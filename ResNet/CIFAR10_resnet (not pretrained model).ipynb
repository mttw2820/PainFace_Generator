{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN ResNet 분류기 구현 testcode\n",
    "### [3분 딥러닝 참고](https://github.com/keon/3-min-pytorch/blob/master/05-%EC%9D%B4%EB%AF%B8%EC%A7%80_%EC%B2%98%EB%A6%AC%EB%8A%A5%EB%A0%A5%EC%9D%B4_%ED%83%81%EC%9B%94%ED%95%9C_CNN/resnet.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터\n",
    "EPOCHS = 300\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./.data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9be00ad0d445dcb3ec2da93c3aa013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./.data\\cifar-10-python.tar.gz to ./.data\n"
     ]
    }
   ],
   "source": [
    "# dataset loader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('./.data',\n",
    "                    train = True,\n",
    "                    download = True,\n",
    "                    transform = transforms.Compose([\n",
    "                        transforms.RandomCrop(32, padding=4),\n",
    "                        transforms.RandomHorizontalFlip(),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                             (0.5, 0.5, 0.5))])),\n",
    "    batch_size = BATCH_SIZE, shuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('./.data',\n",
    "                    train = False,\n",
    "                    transform = transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                             (0.5, 0.5, 0.5))])),\n",
    "    batch_size = BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet Model\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride = 1) :\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size = 3,\n",
    "                               stride = stride, padding = 1, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size = 3,\n",
    "                               stride = 1, padding = 1, bias = False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes :\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size = 1,\n",
    "                          stride = stride, bias = False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x) :\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "    \n",
    "class ResNet(nn.Module) :\n",
    "    def __init__(self, num_classes = 10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size = 3,\n",
    "                               stride = 1, padding = 1, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(16, 2, stride = 1)\n",
    "        self.layer2 = self._make_layer(32, 2, stride = 2)\n",
    "        self.layer3 = self._make_layer(64, 2, stride = 2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "        \n",
    "    def _make_layer(self, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides :\n",
    "            layers.append(BasicBlock(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet().to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.1,\n",
    "                      momentum = 0.9, weight_decay = 0.0005)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 50, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output= model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tets_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad() :\n",
    "        for data, target in test_loader :\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            \n",
    "            # batch 오차 합산\n",
    "            test_loss += F.cross_entropy(output, target,\n",
    "                                         reduction = 'sum').item()\n",
    "            \n",
    "            # 가장 높은 값을 가진 인덱스를 예측값으로\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SM-PC\\Anaconda3\\envs\\py364\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] TEST Loss : 1.4717, Accuracy: 45.60%\n",
      "[2] TEST Loss : 1.3717, Accuracy: 54.05%\n",
      "[3] TEST Loss : 1.0294, Accuracy: 64.03%\n",
      "[4] TEST Loss : 1.1823, Accuracy: 62.65%\n",
      "[5] TEST Loss : 0.8297, Accuracy: 71.83%\n",
      "[6] TEST Loss : 0.7904, Accuracy: 73.51%\n",
      "[7] TEST Loss : 0.7620, Accuracy: 73.99%\n",
      "[8] TEST Loss : 0.9641, Accuracy: 67.82%\n",
      "[9] TEST Loss : 0.8458, Accuracy: 71.70%\n",
      "[10] TEST Loss : 0.6071, Accuracy: 79.09%\n",
      "[11] TEST Loss : 1.3084, Accuracy: 64.21%\n",
      "[12] TEST Loss : 0.7514, Accuracy: 75.83%\n",
      "[13] TEST Loss : 0.8609, Accuracy: 73.29%\n",
      "[14] TEST Loss : 0.6999, Accuracy: 76.30%\n",
      "[15] TEST Loss : 1.0367, Accuracy: 69.03%\n",
      "[16] TEST Loss : 0.8023, Accuracy: 75.08%\n",
      "[17] TEST Loss : 0.6402, Accuracy: 78.35%\n",
      "[18] TEST Loss : 0.7079, Accuracy: 76.43%\n",
      "[19] TEST Loss : 1.0155, Accuracy: 70.10%\n",
      "[20] TEST Loss : 0.6354, Accuracy: 78.52%\n",
      "[21] TEST Loss : 0.7061, Accuracy: 76.37%\n",
      "[22] TEST Loss : 0.6248, Accuracy: 78.14%\n",
      "[23] TEST Loss : 0.5535, Accuracy: 81.25%\n",
      "[24] TEST Loss : 0.8312, Accuracy: 73.53%\n",
      "[25] TEST Loss : 0.9474, Accuracy: 69.01%\n",
      "[26] TEST Loss : 0.6533, Accuracy: 78.53%\n",
      "[27] TEST Loss : 0.6572, Accuracy: 77.83%\n",
      "[28] TEST Loss : 0.6734, Accuracy: 77.81%\n",
      "[29] TEST Loss : 0.7491, Accuracy: 75.63%\n",
      "[30] TEST Loss : 0.6839, Accuracy: 77.75%\n",
      "[31] TEST Loss : 0.7052, Accuracy: 77.69%\n",
      "[32] TEST Loss : 1.0855, Accuracy: 68.62%\n",
      "[33] TEST Loss : 0.6068, Accuracy: 80.17%\n",
      "[34] TEST Loss : 0.6899, Accuracy: 76.25%\n",
      "[35] TEST Loss : 0.5751, Accuracy: 80.32%\n",
      "[36] TEST Loss : 0.5529, Accuracy: 80.96%\n",
      "[37] TEST Loss : 0.6274, Accuracy: 79.18%\n",
      "[38] TEST Loss : 0.7692, Accuracy: 75.75%\n",
      "[39] TEST Loss : 0.5926, Accuracy: 79.61%\n",
      "[40] TEST Loss : 0.6693, Accuracy: 78.01%\n",
      "[41] TEST Loss : 0.6273, Accuracy: 78.68%\n",
      "[42] TEST Loss : 0.7783, Accuracy: 75.33%\n",
      "[43] TEST Loss : 0.7547, Accuracy: 75.03%\n",
      "[44] TEST Loss : 0.6158, Accuracy: 79.46%\n",
      "[45] TEST Loss : 0.6434, Accuracy: 78.83%\n",
      "[46] TEST Loss : 0.7131, Accuracy: 76.49%\n",
      "[47] TEST Loss : 0.6812, Accuracy: 77.59%\n",
      "[48] TEST Loss : 0.7302, Accuracy: 76.95%\n",
      "[49] TEST Loss : 0.6418, Accuracy: 77.63%\n",
      "[50] TEST Loss : 0.3417, Accuracy: 88.30%\n",
      "[51] TEST Loss : 0.3362, Accuracy: 88.41%\n",
      "[52] TEST Loss : 0.3242, Accuracy: 89.10%\n",
      "[53] TEST Loss : 0.3279, Accuracy: 88.72%\n",
      "[54] TEST Loss : 0.3241, Accuracy: 89.12%\n",
      "[55] TEST Loss : 0.3357, Accuracy: 88.58%\n",
      "[56] TEST Loss : 0.3248, Accuracy: 88.77%\n",
      "[57] TEST Loss : 0.3228, Accuracy: 89.23%\n",
      "[58] TEST Loss : 0.3112, Accuracy: 89.50%\n",
      "[59] TEST Loss : 0.3229, Accuracy: 88.97%\n",
      "[60] TEST Loss : 0.3173, Accuracy: 89.53%\n",
      "[61] TEST Loss : 0.3223, Accuracy: 89.07%\n",
      "[62] TEST Loss : 0.3278, Accuracy: 89.30%\n",
      "[63] TEST Loss : 0.3270, Accuracy: 89.32%\n",
      "[64] TEST Loss : 0.3671, Accuracy: 88.24%\n",
      "[65] TEST Loss : 0.3413, Accuracy: 88.63%\n",
      "[66] TEST Loss : 0.3310, Accuracy: 89.07%\n",
      "[67] TEST Loss : 0.3373, Accuracy: 88.86%\n",
      "[68] TEST Loss : 0.3570, Accuracy: 88.47%\n",
      "[69] TEST Loss : 0.3253, Accuracy: 89.22%\n",
      "[70] TEST Loss : 0.3576, Accuracy: 88.28%\n",
      "[71] TEST Loss : 0.3348, Accuracy: 88.87%\n",
      "[72] TEST Loss : 0.3496, Accuracy: 88.31%\n",
      "[73] TEST Loss : 0.3628, Accuracy: 88.18%\n",
      "[74] TEST Loss : 0.3569, Accuracy: 88.73%\n",
      "[75] TEST Loss : 0.3619, Accuracy: 88.26%\n",
      "[76] TEST Loss : 0.3369, Accuracy: 89.03%\n",
      "[77] TEST Loss : 0.3769, Accuracy: 87.97%\n",
      "[78] TEST Loss : 0.3640, Accuracy: 88.48%\n",
      "[79] TEST Loss : 0.3751, Accuracy: 87.73%\n",
      "[80] TEST Loss : 0.3626, Accuracy: 88.26%\n",
      "[81] TEST Loss : 0.3681, Accuracy: 87.84%\n",
      "[82] TEST Loss : 0.3491, Accuracy: 88.86%\n",
      "[83] TEST Loss : 0.3750, Accuracy: 87.98%\n",
      "[84] TEST Loss : 0.3700, Accuracy: 87.92%\n",
      "[85] TEST Loss : 0.3413, Accuracy: 88.76%\n",
      "[86] TEST Loss : 0.3714, Accuracy: 88.04%\n",
      "[87] TEST Loss : 0.3583, Accuracy: 88.22%\n",
      "[88] TEST Loss : 0.3672, Accuracy: 88.01%\n",
      "[89] TEST Loss : 0.3704, Accuracy: 88.33%\n",
      "[90] TEST Loss : 0.3730, Accuracy: 87.90%\n",
      "[91] TEST Loss : 0.3809, Accuracy: 88.06%\n",
      "[92] TEST Loss : 0.3744, Accuracy: 88.56%\n",
      "[93] TEST Loss : 0.3450, Accuracy: 89.19%\n",
      "[94] TEST Loss : 0.3517, Accuracy: 88.67%\n",
      "[95] TEST Loss : 0.3909, Accuracy: 87.89%\n",
      "[96] TEST Loss : 0.3530, Accuracy: 88.57%\n",
      "[97] TEST Loss : 0.4246, Accuracy: 86.73%\n",
      "[98] TEST Loss : 0.3723, Accuracy: 88.02%\n",
      "[99] TEST Loss : 0.3556, Accuracy: 88.09%\n",
      "[100] TEST Loss : 0.2892, Accuracy: 90.53%\n",
      "[101] TEST Loss : 0.2845, Accuracy: 90.70%\n",
      "[102] TEST Loss : 0.2848, Accuracy: 90.50%\n",
      "[103] TEST Loss : 0.2813, Accuracy: 90.57%\n",
      "[104] TEST Loss : 0.2837, Accuracy: 90.81%\n",
      "[105] TEST Loss : 0.2799, Accuracy: 90.77%\n",
      "[106] TEST Loss : 0.2801, Accuracy: 91.05%\n",
      "[107] TEST Loss : 0.2822, Accuracy: 91.06%\n",
      "[108] TEST Loss : 0.2804, Accuracy: 91.05%\n",
      "[109] TEST Loss : 0.2793, Accuracy: 91.10%\n",
      "[110] TEST Loss : 0.2806, Accuracy: 91.12%\n",
      "[111] TEST Loss : 0.2800, Accuracy: 91.09%\n",
      "[112] TEST Loss : 0.2815, Accuracy: 90.98%\n",
      "[113] TEST Loss : 0.2852, Accuracy: 90.91%\n",
      "[114] TEST Loss : 0.2827, Accuracy: 90.96%\n",
      "[115] TEST Loss : 0.2831, Accuracy: 91.11%\n",
      "[116] TEST Loss : 0.2855, Accuracy: 91.00%\n",
      "[117] TEST Loss : 0.2808, Accuracy: 91.24%\n",
      "[118] TEST Loss : 0.2871, Accuracy: 90.98%\n",
      "[119] TEST Loss : 0.2905, Accuracy: 90.70%\n",
      "[120] TEST Loss : 0.2875, Accuracy: 90.93%\n",
      "[121] TEST Loss : 0.2905, Accuracy: 90.93%\n",
      "[122] TEST Loss : 0.2831, Accuracy: 91.18%\n",
      "[123] TEST Loss : 0.2824, Accuracy: 91.06%\n",
      "[124] TEST Loss : 0.2874, Accuracy: 91.14%\n",
      "[125] TEST Loss : 0.2904, Accuracy: 90.87%\n",
      "[126] TEST Loss : 0.2898, Accuracy: 91.11%\n",
      "[127] TEST Loss : 0.2841, Accuracy: 91.26%\n",
      "[128] TEST Loss : 0.2904, Accuracy: 91.15%\n",
      "[129] TEST Loss : 0.2880, Accuracy: 91.08%\n",
      "[130] TEST Loss : 0.2903, Accuracy: 90.93%\n",
      "[131] TEST Loss : 0.2952, Accuracy: 90.74%\n",
      "[132] TEST Loss : 0.2905, Accuracy: 91.02%\n",
      "[133] TEST Loss : 0.2929, Accuracy: 90.84%\n",
      "[134] TEST Loss : 0.2874, Accuracy: 91.07%\n",
      "[135] TEST Loss : 0.2922, Accuracy: 91.14%\n",
      "[136] TEST Loss : 0.2961, Accuracy: 90.81%\n",
      "[137] TEST Loss : 0.2949, Accuracy: 91.21%\n",
      "[138] TEST Loss : 0.2965, Accuracy: 91.09%\n",
      "[139] TEST Loss : 0.2924, Accuracy: 91.02%\n",
      "[140] TEST Loss : 0.2957, Accuracy: 90.93%\n",
      "[141] TEST Loss : 0.2993, Accuracy: 91.05%\n",
      "[142] TEST Loss : 0.2964, Accuracy: 90.92%\n",
      "[143] TEST Loss : 0.2937, Accuracy: 91.03%\n",
      "[144] TEST Loss : 0.2952, Accuracy: 90.87%\n",
      "[145] TEST Loss : 0.2998, Accuracy: 90.64%\n",
      "[146] TEST Loss : 0.2974, Accuracy: 90.99%\n",
      "[147] TEST Loss : 0.2996, Accuracy: 90.80%\n",
      "[148] TEST Loss : 0.2978, Accuracy: 91.04%\n",
      "[149] TEST Loss : 0.3025, Accuracy: 90.82%\n",
      "[150] TEST Loss : 0.2982, Accuracy: 90.86%\n",
      "[151] TEST Loss : 0.2973, Accuracy: 90.99%\n",
      "[152] TEST Loss : 0.2960, Accuracy: 90.94%\n",
      "[153] TEST Loss : 0.2974, Accuracy: 90.95%\n",
      "[154] TEST Loss : 0.2953, Accuracy: 90.93%\n",
      "[155] TEST Loss : 0.2952, Accuracy: 91.00%\n",
      "[156] TEST Loss : 0.2966, Accuracy: 90.94%\n",
      "[157] TEST Loss : 0.2933, Accuracy: 90.98%\n",
      "[158] TEST Loss : 0.2931, Accuracy: 91.08%\n",
      "[159] TEST Loss : 0.2937, Accuracy: 91.07%\n",
      "[160] TEST Loss : 0.2939, Accuracy: 91.03%\n",
      "[161] TEST Loss : 0.2952, Accuracy: 90.95%\n",
      "[162] TEST Loss : 0.2950, Accuracy: 91.06%\n",
      "[163] TEST Loss : 0.2949, Accuracy: 91.02%\n",
      "[164] TEST Loss : 0.2955, Accuracy: 90.99%\n",
      "[165] TEST Loss : 0.2934, Accuracy: 91.19%\n",
      "[166] TEST Loss : 0.2942, Accuracy: 91.09%\n",
      "[167] TEST Loss : 0.2964, Accuracy: 91.12%\n",
      "[168] TEST Loss : 0.2958, Accuracy: 90.98%\n",
      "[169] TEST Loss : 0.2970, Accuracy: 91.14%\n",
      "[170] TEST Loss : 0.2981, Accuracy: 91.11%\n",
      "[171] TEST Loss : 0.2933, Accuracy: 91.05%\n",
      "[172] TEST Loss : 0.2961, Accuracy: 91.15%\n",
      "[173] TEST Loss : 0.2967, Accuracy: 91.12%\n",
      "[174] TEST Loss : 0.2962, Accuracy: 91.12%\n",
      "[175] TEST Loss : 0.2933, Accuracy: 91.07%\n",
      "[176] TEST Loss : 0.2964, Accuracy: 91.07%\n",
      "[177] TEST Loss : 0.2947, Accuracy: 91.11%\n",
      "[178] TEST Loss : 0.2943, Accuracy: 91.14%\n",
      "[179] TEST Loss : 0.2949, Accuracy: 91.05%\n",
      "[180] TEST Loss : 0.2974, Accuracy: 91.05%\n",
      "[181] TEST Loss : 0.2954, Accuracy: 91.15%\n",
      "[182] TEST Loss : 0.2949, Accuracy: 90.96%\n",
      "[183] TEST Loss : 0.2948, Accuracy: 91.04%\n",
      "[184] TEST Loss : 0.2971, Accuracy: 91.05%\n",
      "[185] TEST Loss : 0.2969, Accuracy: 91.18%\n",
      "[186] TEST Loss : 0.3013, Accuracy: 90.97%\n",
      "[187] TEST Loss : 0.2965, Accuracy: 91.14%\n",
      "[188] TEST Loss : 0.2945, Accuracy: 91.16%\n",
      "[189] TEST Loss : 0.2968, Accuracy: 91.15%\n",
      "[190] TEST Loss : 0.2973, Accuracy: 91.01%\n",
      "[191] TEST Loss : 0.2957, Accuracy: 91.08%\n",
      "[192] TEST Loss : 0.2962, Accuracy: 90.99%\n",
      "[193] TEST Loss : 0.2950, Accuracy: 91.10%\n",
      "[194] TEST Loss : 0.2956, Accuracy: 91.18%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[195] TEST Loss : 0.2954, Accuracy: 91.13%\n",
      "[196] TEST Loss : 0.2957, Accuracy: 91.07%\n",
      "[197] TEST Loss : 0.2973, Accuracy: 90.97%\n",
      "[198] TEST Loss : 0.2958, Accuracy: 90.97%\n",
      "[199] TEST Loss : 0.2952, Accuracy: 91.12%\n",
      "[200] TEST Loss : 0.2967, Accuracy: 91.01%\n",
      "[201] TEST Loss : 0.2963, Accuracy: 91.02%\n",
      "[202] TEST Loss : 0.2978, Accuracy: 91.04%\n",
      "[203] TEST Loss : 0.2987, Accuracy: 90.98%\n",
      "[204] TEST Loss : 0.2966, Accuracy: 91.00%\n",
      "[205] TEST Loss : 0.2957, Accuracy: 91.07%\n",
      "[206] TEST Loss : 0.2985, Accuracy: 91.04%\n",
      "[207] TEST Loss : 0.2973, Accuracy: 91.06%\n",
      "[208] TEST Loss : 0.2972, Accuracy: 91.07%\n",
      "[209] TEST Loss : 0.2961, Accuracy: 90.97%\n",
      "[210] TEST Loss : 0.2972, Accuracy: 91.12%\n",
      "[211] TEST Loss : 0.2961, Accuracy: 90.89%\n",
      "[212] TEST Loss : 0.2962, Accuracy: 91.06%\n",
      "[213] TEST Loss : 0.2957, Accuracy: 91.08%\n",
      "[214] TEST Loss : 0.2963, Accuracy: 91.09%\n",
      "[215] TEST Loss : 0.2969, Accuracy: 91.07%\n",
      "[216] TEST Loss : 0.2971, Accuracy: 91.10%\n",
      "[217] TEST Loss : 0.2970, Accuracy: 91.15%\n",
      "[218] TEST Loss : 0.2979, Accuracy: 91.11%\n",
      "[219] TEST Loss : 0.2974, Accuracy: 90.99%\n",
      "[220] TEST Loss : 0.2963, Accuracy: 91.09%\n",
      "[221] TEST Loss : 0.2941, Accuracy: 90.89%\n",
      "[222] TEST Loss : 0.2959, Accuracy: 91.07%\n",
      "[223] TEST Loss : 0.2968, Accuracy: 91.00%\n",
      "[224] TEST Loss : 0.2954, Accuracy: 91.02%\n",
      "[225] TEST Loss : 0.2952, Accuracy: 91.09%\n",
      "[226] TEST Loss : 0.2959, Accuracy: 91.00%\n",
      "[227] TEST Loss : 0.2956, Accuracy: 91.12%\n",
      "[228] TEST Loss : 0.2961, Accuracy: 91.13%\n",
      "[229] TEST Loss : 0.2966, Accuracy: 90.98%\n",
      "[230] TEST Loss : 0.2957, Accuracy: 91.07%\n",
      "[231] TEST Loss : 0.2962, Accuracy: 91.02%\n",
      "[232] TEST Loss : 0.2964, Accuracy: 91.04%\n",
      "[233] TEST Loss : 0.2968, Accuracy: 91.07%\n",
      "[234] TEST Loss : 0.2965, Accuracy: 91.02%\n",
      "[235] TEST Loss : 0.2990, Accuracy: 90.93%\n",
      "[236] TEST Loss : 0.2968, Accuracy: 91.09%\n",
      "[237] TEST Loss : 0.2984, Accuracy: 90.93%\n",
      "[238] TEST Loss : 0.2973, Accuracy: 90.93%\n",
      "[239] TEST Loss : 0.2968, Accuracy: 91.01%\n",
      "[240] TEST Loss : 0.2958, Accuracy: 91.06%\n",
      "[241] TEST Loss : 0.2975, Accuracy: 90.92%\n",
      "[242] TEST Loss : 0.2978, Accuracy: 90.98%\n",
      "[243] TEST Loss : 0.2958, Accuracy: 91.08%\n",
      "[244] TEST Loss : 0.2970, Accuracy: 91.08%\n",
      "[245] TEST Loss : 0.2956, Accuracy: 91.09%\n",
      "[246] TEST Loss : 0.2961, Accuracy: 91.00%\n",
      "[247] TEST Loss : 0.2963, Accuracy: 91.10%\n",
      "[248] TEST Loss : 0.2964, Accuracy: 91.06%\n",
      "[249] TEST Loss : 0.2973, Accuracy: 91.11%\n",
      "[250] TEST Loss : 0.2966, Accuracy: 91.01%\n",
      "[251] TEST Loss : 0.2964, Accuracy: 90.98%\n",
      "[252] TEST Loss : 0.2999, Accuracy: 91.12%\n",
      "[253] TEST Loss : 0.2958, Accuracy: 91.06%\n",
      "[254] TEST Loss : 0.2961, Accuracy: 91.10%\n",
      "[255] TEST Loss : 0.2959, Accuracy: 91.11%\n",
      "[256] TEST Loss : 0.2977, Accuracy: 91.05%\n",
      "[257] TEST Loss : 0.2959, Accuracy: 91.11%\n",
      "[258] TEST Loss : 0.2961, Accuracy: 90.91%\n",
      "[259] TEST Loss : 0.2988, Accuracy: 91.00%\n",
      "[260] TEST Loss : 0.2969, Accuracy: 90.99%\n",
      "[261] TEST Loss : 0.2956, Accuracy: 91.04%\n",
      "[262] TEST Loss : 0.2966, Accuracy: 91.09%\n",
      "[263] TEST Loss : 0.2965, Accuracy: 91.08%\n",
      "[264] TEST Loss : 0.2944, Accuracy: 91.13%\n",
      "[265] TEST Loss : 0.2971, Accuracy: 91.01%\n",
      "[266] TEST Loss : 0.2950, Accuracy: 91.09%\n",
      "[267] TEST Loss : 0.2962, Accuracy: 91.11%\n",
      "[268] TEST Loss : 0.2960, Accuracy: 91.04%\n",
      "[269] TEST Loss : 0.2979, Accuracy: 91.06%\n",
      "[270] TEST Loss : 0.2976, Accuracy: 91.00%\n",
      "[271] TEST Loss : 0.2972, Accuracy: 90.95%\n",
      "[272] TEST Loss : 0.2972, Accuracy: 91.06%\n",
      "[273] TEST Loss : 0.2952, Accuracy: 90.98%\n",
      "[274] TEST Loss : 0.2977, Accuracy: 90.98%\n",
      "[275] TEST Loss : 0.2967, Accuracy: 90.96%\n",
      "[276] TEST Loss : 0.2968, Accuracy: 91.11%\n",
      "[277] TEST Loss : 0.2954, Accuracy: 91.00%\n",
      "[278] TEST Loss : 0.2958, Accuracy: 91.04%\n",
      "[279] TEST Loss : 0.2979, Accuracy: 90.81%\n",
      "[280] TEST Loss : 0.2959, Accuracy: 91.12%\n",
      "[281] TEST Loss : 0.2988, Accuracy: 91.10%\n",
      "[282] TEST Loss : 0.2967, Accuracy: 91.08%\n",
      "[283] TEST Loss : 0.2977, Accuracy: 91.05%\n",
      "[284] TEST Loss : 0.2945, Accuracy: 91.19%\n",
      "[285] TEST Loss : 0.2961, Accuracy: 91.12%\n",
      "[286] TEST Loss : 0.2971, Accuracy: 91.07%\n",
      "[287] TEST Loss : 0.2983, Accuracy: 91.01%\n",
      "[288] TEST Loss : 0.2972, Accuracy: 91.13%\n",
      "[289] TEST Loss : 0.2964, Accuracy: 91.09%\n",
      "[290] TEST Loss : 0.2968, Accuracy: 91.12%\n",
      "[291] TEST Loss : 0.2970, Accuracy: 91.11%\n",
      "[292] TEST Loss : 0.2969, Accuracy: 91.04%\n",
      "[293] TEST Loss : 0.2976, Accuracy: 91.05%\n",
      "[294] TEST Loss : 0.2971, Accuracy: 91.00%\n",
      "[295] TEST Loss : 0.2978, Accuracy: 91.03%\n",
      "[296] TEST Loss : 0.2968, Accuracy: 91.02%\n",
      "[297] TEST Loss : 0.2979, Accuracy: 91.00%\n",
      "[298] TEST Loss : 0.2996, Accuracy: 90.86%\n",
      "[299] TEST Loss : 0.2945, Accuracy: 90.96%\n",
      "[300] TEST Loss : 0.2966, Accuracy: 90.99%\n"
     ]
    }
   ],
   "source": [
    "# 실행\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    scheduler.step()\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    \n",
    "    print('[{}] TEST Loss : {:.4f}, Accuracy: {:.2f}%'.format(\n",
    "            epoch, test_loss, test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
